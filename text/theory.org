#+latex_class: article_usual2
# erases make title
#+BIND: org-export-latex-title-command ""

# fucks all the maketitlestuff just to be sure
#+OPTIONS: num:nil
#+OPTIONS: toc:nil
# #+OPTIONS: toc:nil#+TITLE: #+AUTHOR: #+DATE: 
#+OPTIONS: h:5

# -*- org-export-babel-evaluate: nil -*-



* theory/IRC final

** notes alex
DV: survival or abandonment of cognitive labels used by audiences. 

Mechanisms: 
- Conceptual informativeness or distinctiveness. -> using the musicological features of songs.  
- Distinctiveness: Piazzai approach. MDS of features. Songs. Spherical space. 
- Informativeness: hierarchical concept. nature of the relations among concepts. 
- Audience niche size. 
- Audience composition: avant-garde or mainstream. 
- Status: average Billboard. Label could be more widely applied in aesthetic and social space. Expansion of use of genre labels due to increased popularity.
- Density measures. 
- Legitimation= same as status. 
- Rapid adoption could lead to abandonment. 

Methodological:
- assymmetry: 
- identification of genre-labels
- nature of the relations among concepts. 
- how tightly connceted the audiences are? two-mode not well connected individual listeners. 
- length of the time period. 
- Billboard. 


** own notes
unit of analysis is concept -> start with concepts

ecological 

add stuff about partiality 
- mean
  - high: salient
  - low: peripheral
- skew:
  - high: concentration
  - low: equality

can there be low mean but high skew? 

theoretical implications: 

*** fuzziness
concept (fruit) is fuzzy if there are objects (olive) of which it is not clear whether it belongs or not

classical category: just 100s (rest 0s)
somewhat fuzzy: many 100s, some non 100s

very fuzzy: many non 100s

the lower the average weight, the fuzzier the category? 

electronic: will be more fuzzy than syncwave-electrofunk
but that's accurate isn't it? 
people who use a very specific label know it well, have clearer criteria -> can make membership more binary

*** informativeness
needs hierarchy!!!

average informativeness?

need way to formalize informativeness from asymmetric competition coefs

hierarchy is a way to deal with asymmetry
Tversky thinks so too: [[cite:Smith_1981_categories][p.118]]

make the use more the improvisations (Bourdieu) -> stresses flexibility

*** other areas of concepts: 
- psychology
- neurophysiology
- information science

* text
** introduction
#

** concepts
# 
With genres as the unit of analysis, ~it is necessary~ to expand on general features of these mental blueprints.
#

*** fuzziness/partial membership
textcite:Hannan_2019_concepts provide a summary of the recent concept literature and highlight a number of key properties. 
#
~Categories~

Concepts are fuzzy: 

Given
- logocentrism (Derrida)
- bureaucratization/iron cage 
  - narrow down to institutional logic? 

It may seem intuitive to think that concepts can clearly described with a precise definition of features. 

textcite:Murphy_2004_concepts argues that the "classical view" in the psychology of concepts constituted such a praradigm  until the 1970s considered concepts as definitions with necessary and jointly sufficient attributes. 
# 
While there have always been philosophical criticisms of the non-universality of rationality 
- Wittgenstein
- Habermas
a paradigm shift took place in psychology only with 
- ~experiments~.


textcite:Murphy_2004_concepts,Hannan_2019_concepts argue that concepts are fuzzy and entail blurred boundaries. 
# 
Consequentely no clear defining list of necessary and/or sufficient features can exist for a concept. 
# 
For example, while there are features which we strongly associate with the concept of a ~dog~  such as fur, barking, or four legs, we can well imagine animals that do not fulfill all of these attributes but that we would nevertheless call dogs. 
# ~Wittgenstein game example?~
# 
Such lack of logical clarity and hence debate over terminologies regarding membership criteria are present in virtually every context, including highly technical ones such as astronomy (whether Pluto is a planet) or mathematics (classification of polyhedras, cite:Lakatos_2015_proofs)


~need to account for partial membership because data (tag weights)~
use fruits; tomato as illustrations



# 
The abandonment of crisp boundaries does not entail the adoption of (caricature of) an arbitrary 'postmodern' 'anything-goes' approach. 
# 
Instead, cite:Hannan_2019_concepts invoke the notion of "probability densities" to capture the variation of the probability of an objects to belong to concept based on their position in a feature space. 
# 
The relations between concepts can hence be inferred by the relations of their respective probability densities. 
#
Concepts that occupy similar regions of the feature space (such as ~sociologist~ and ~anthropologist~) are ~closer together~ than concepts that reside in different regions (such as ~sociologist~ and ~lumberjack~). 
# this is relevant how? 




*** hierarchy
A key issue concerns the hierarchy of concepts. 
#
As concepts vary in their degree of abstraction (e.g vehicle -> car -> pickup truck), different approaches exist to account for these different levels. 
#
The ~classical view (is it really the classical view?)~ sees concepts stored in an hierarchical network, where at each level only the relevant features are stored. 
#
For example in the case of vehicles, the property of being a mechanical device to transport things is seen as associated with the most general ~vehicle~ level, the feature of having wheels and driving on roads to the intermediate and the feature of having an open cargo area associated with the most specific level. 
#
Within such a (crisp) hierarchy, features are inherited from higher levels. 
# 
Membership ~in the sub-concepts~ is furthermore transitive: all pick up trucks are cars as well as vehicles. 
# 




cite:Hannan_2019_concepts to a large extent follow cite:Collins_1969_retrieval,Quillian_1988_memory in conceptualizing concepts as stored explicitly in a hierarchical network. 
#
They conceptualize the resulting conceptual space as a semi-lattice, a nested tree built from sub-concept relations. 
# 
In this directed tree, conceptual /roots/ (e.g. vehicles) spawn /cohorts/ of concepts (e.g. car, ship, plane); groups of concepts at the same level of abstraction. 
#
They nevertheless diverge from the classical to some extent. 
# 
First, they consider feature inheritance as probabilistic, as more typical subconcepts inherit more features ~and feature dimensions~ from their roots than atypical ones. 
#
Second, they explicitly allow inheritance from multiple parents, which takes into accounts 'hybrids' such as romantic comedy or ~flying cars~. 



However, it seems to me that despite these additions the assumption of explicitly stored hierarchical links is not consistent with theoretical considerations and empirical evidence. 
#
Instead, I find more convincing the argument of textcite:Murphy_2004_concepts that hierarchical links are not explicitly stored, but computed based on similarity values. 
#
One reason is transitivity of membership. 
#
Murphy (refering to ~source~) argues that when subjects agree that chairs are a type of furniture, and car seats are a type of chairs, they nevertheless do not consider car seats a type of furniture. 
#
He argues that the features that car seat shares with chair are different from those that chair shares with furniture, which leads to refusal of membership even if it would follow on logical grounds. 
#
A further argument against explicitly stored hierarchical links are response times. 
#
If features are only stored at one level, inferences would take the longer the more hierarchical links they have to traverse through. 
# 
While this holds in some cases ~page~, it can also be violated: 
# 
cite:Murphy_2004_concepts and ~other source (Rips et al)~ find that verifying the statement 'a dog is an animal' is faster than the verifying the statement 'a dog is a mammal'. 
# 
If hierarchical links were explicitly stored, the latter statement would be faster as the mammal concept would be closer to the dog concept than the animal concept of which it is a subconcept of. 
# 
Murphy explains this with dogs being more typical animals than mammals. 
# wouldn't it need to be: dogs are more typical of animals than they are of mammals? 


# distance in conceptual space: smart and pickup truck are further apart (have to link through car) than each with car




cite:Hannan_2019_concepts are aware of the latter argument, and in response argue that features duplicated at lower levels. 
# 
As this feature replication is based on typicality and therefore probabilistical it is compatible with variation in inference time based on typicality. 
#
I however do not find this satisfactory: 
# 
It breaks with the idea of cognitive economy (~Rosch~) of hierarchies as an efficient storage of conceptual information. 
#
It also renders the hierarchical links ineffective: 
# 
Since ~Hannan~ argue that features are passed down (not conceptual memberships; i.e. a dog has animal and mammal features, but not the concept labels), it turns questions of membership effectively into computation: 
# 
- Since the goal of feature replication is to avoid having to travers the hierarchical pathways, 
- Rather than traversing the hierarchical paths, 
the question of whether a dog is a an animal would then be solved by comparing the features of a dog (which now includes those of mammals and animals) with that of an animal. 
#
It also appears to me to be an add-hoc addition to explain empirical observations rather than being the result of theoretical necessity. 

Hannan quote: 

Suppose for simplicity that the concept c1 at highest level in the branch of a tree has a very high likelihood for possessing the property (a value of a binary feature) A, that its direct descendant c2 adds a high likelihood for the property B and therefore has high likelihoods for the properties {A, B}, and that a still-lower level concept c3 adds high likelihood for the property C, which means that it has high likelihood for the properties {A, B, C}. Now suppose that (for some reason) we have not been able to observe for instances of c3 whether or not they possess the property A. [...] We could have observed all of the properties of (prototypical) instances of c2 and learned that they possess A (as is highly likely in this example). The inference can stop there, there is no need to go higher.

However, if we assume we have instances c3, we already know that (with high certainty) that they have property A simply by the fact that we know them as instances of c3. 
#
For example, if we have a Pickup truck (of which we know that it is an actual pickup truck and not a museum piece the engine of which been removed) and assume that properties are replicated, we do not need to make inferences to the car level, but can /per definition/ assume that the the pickup truck can move things or people. 


# therefore argues against a hierarchical taxonomy 


cite:Murphy_2004_concepts however rejects Q.'s approach and argues that hierarchies are imputed based on typicality values: 
- faster that dog is animal than dog is mammal: dog is more typical animal than mammal
- furniture; chair: car seat, : transitivity violation: features that car seat shares with chair are different from those that chair shares with furniture



# dog-> animal is FASTER than dog -> mammal, not just equal 
# Even if animal features are completely replicated, inference should at most be same speed
# dog could get animal features from somehwere else: multiple parents
# pets: pets are very typical animals
# what is the relation between pets and mammals? 

# hmm: 
# dogs are very typical pets
# but not every dog is a pet -> statement: 'a dog is a pet' is ambiguous, would need to be more specific (all dogs -> false, some dogs -> true)

# i think this is the case that Murphy doesn't cover: focuses on hierarchies, not possible ways
# pets contrast with farm animals and wild animals
# mammals contrast with reptiles, bird, fish
# mammal features not passed down as much as animal features
# one might assume that inferences from lowest to general (dog -> animal) could not be faster than from lowest to medium (dog -> mammal). 

But even if features are replicated at lower levels, 
- I cannot think of a reason why
- it might seem implausible that
inferences from lower to general (dog -> animal) could be /faster/ than from low to medium (dog -> mammal).
#
Two different explanations might be possible: 
#
Mammal features might not be passed on as much as animal features, which might be accurate as dogs are not typical mammals (most of interaction with and/or public imagination of dogs is unrelated to mammal characteristics).
# 
Alternatively, animal features might be passed down from a different category, for example ~pets~ (which, as they are no subcategory of mammals, do not pass down mammal features. 
#
This would require further specification on the relationship between the concepts mammals and pets. 
#
While both are sub-concepts of animals, they do not seem to be part of the same cohort: 
# 
Mammals as a term for taxonomic classification would be in a cohort with concepts such as fish and reptiles, while pets, which specifies the relation to humans, would be in the same cohort as farm animals and wild animals. 
#
As such a dog is not a hybrid in the same sense as a romantic comedy is as the concepts belong to different "visions of division" cite:Bourdieu_1989_space (~different things at stake~).
# 
These issues are not yet discussed by textcite:Murphy_2004_concepts who mentions such cases, but limit his discussion of multiple membership to hierarchically structured taxonomies (p.199), or textcite:Hannan_2019_concepts, who discuss ambiguity due to multiple membership with regards to concepts of the same cohort (p.152). 



*** pieces



# feature spaces need to be linked: 
# need to be able to ask: what is the difference between an economist and a sociologist
# can't be that each has its own separate feature space
Hannan: differences between concepts really explicitly specified
more elegant: differences between concepts in terms of features -> two mode network? 




To describe the relation of concepts to other concepts, the authors employ the notion of a conceptual space. 
#
They argue that concepts are hierarchically structured based on their level of abstraction , 
just not mention informativeness in main part, just in discussion
do i even need conceptual space then? 





# difference between concept and feature space? 
# - feature space: UoA: objects, dimensions: features
# - conceptual space: UoA: concepts, no dimensions, but distances/is-a relations to other concepts


The variation in the probabilities of category membership are described with typicality (or its reverse, atypicality). 
#
While robins are highly typical members of the bird category, penguins are atypical birds as they lack a feature that we see as important of birds, namely the ability to fly. 
# but are still birds -> binary membership? 
# 
While penguins are considered birds (and hence display discrete membership status in the institutionalized taxonoy), atypicality effects are nonetheless present and influence e.g. the speed ~and accuracy?~ of logical inferences. 



# relationship between: 
# - concepts
# - objects
# - features

# objects fit into concepts/categories with varying fit due to varying fit of their features to the concept prototypes


**** hierarchy
# 
If concepts are conceptualized as probability distributions within a feature space, the relations are not explicitly stored, but computed on the spot. 
# 
While it may seem straightforward to assess the similarity (or distance) between two concepts by the proximity of the centers (or borders) of their probability distribution, ~such as Hannan proposes~,  it seems to me that this conceptualization cannot take into account 
- empirical findings
- theoretical considerations
- recent insights 
regarding the nature of concepts. 
# 
Imagining for example the distance between two concepts as the distance of their centers would result in symmetric distances. 
# 
~While distances from the center of one concept to a region of a particular density of the other concept might~
~problem: asymetric distances work with distances as actual distances between concepts in feature space~
~triangle inequality can be explained: cuba basically is 'conductive': space it occupies doesn't have to be crossed completely by sum of paths (JC, CR), but by JR path~

can i make the argument that conceptual spaces are not real?
spaces are our explanation of explaining, but not what's actually going on in people's head: networks much better for high dimensionality

-> similarity as distance reifies an abstraction? 
maintaining high dimensional space is expensive

if you ask: why are NK and CH similar, you don't get they are similar in high-dimensional space, but that they are countries, Asian, commies, non-democratic, bad

high dimensional space could also be a metaphor for the functional mapping of PDP
but feature links are more realistic/better me thinks

~look up what hannan says about dimensionality of sub-spaces~: 
concepts have different dimensions
for comparisons, the root concept is expanded into the conceptual space of the subconcept

kullback-leibler divergence: is basically measure of overlap: 
doesn't allow any 0s tho -> is not really measure of overlap? 


if geometric distance: 
probability distribution has no impact on similarity judgments
but that's just a theory
test: 
bunch of concepts, need independently : 
- features listing 
- similarity judgements
NOPE: KL divergence takes into account distributions 

Instead, it seems 

**** hierarchy relations
# 
Hanna et al use the Kullback-Leibler divergence (KDV) to measure distance between concepts (p.75), which is calculated as 
#
\begin{equation*}
D_{KL}(P_1||P_2) = \sum\limits_{x \in \mathbb{G}} P_1(x)  \log \left( \frac{P_1(x)}{P_2(x)} \right)
\end{equation*}
#
They however do not mention that KDV is only defined under absolute continuity, which requires that for all x where P_2(x) is zero, P_1(x) likewise has to be zero (~better source than wikipedia~). 
# 
Failure of this restriction would result the fraction to be undefined. 
# 
The opposite case, (P_1 being zero at an x where P_2 is non-zero) does pose less of a problem: 
# 
While log(0) is negative infinite, the entire expression becomes 0 as the following applies: 

\begin{equation*}
\lim \limits_{x \rightarrow 0^{+}} x \log(x) = 0
\end{equation*}
# what is 0+?

~test with zeroes in x1~
As such KDV is suited for measuring the informativeness of a sub-concept in relation to its root, as the criterion ob absolute continuity holds if a sub-concept occupies a sub-region of a conceptual space. 
# 
For example, the divergence of the concept of a ~swimmer~ from its root concept ~athlete~ can be meaningfully calculated in this fashion. 
#
~Here, a swimmer has non-zero values on all attributes of the ~athlete~ category~. 
# 
This is not the case for assessing the similarity between ~swimmer~ and ~weightlifter~, as the probability distributions in the dimensions that define each category (training (primarily) in pool, training (primarily) with weights) are non-overlapping. 

#
It is unclear if  textcite:Hannan_2019_concepts are aware of this limitation. 
# 
When measuring the distance between concepts on the same level in the case of a cohort, they claim to "also use the Kullbach-Leibler divergence to express the degree to which a concept stands out from its cohort" (p.81). 
#
However, when they do so in practice, they use cosine similarities converted into distances using an exponential function (and then average them for each concept as a measure of its distinctiveness vis-a-vis its cohort comembers). 
# 
However, using cosine similarities of (the centers of) the feature space dimensions does not take information of the probability distribution into account. 
# 
It would for example imply that two concepts are equally similar if their centers are at the same distance, regardless of the extent to which their probability distributions overlap (~figure X~), which strikes me as unintuitive. 

It is also a symmetric distance. 
# 
One might argue that in the case of cohorts it is more justifiable to assume equal sizes, the very notion of variation in typicality seems to counteract this idea.
~are penguins more similar to robins than robins to penguins~? 


Using different measurements for these two cases, typicality of ~sub-conceptuality~ and distance between concepts, does not strike me as a bad idea. 
#
The question "how similar are swimmers to athletes" seems to me to be a qualitatively different one than the question "how similar are swimmers to sociologists". 
# 



**** metric spaces bad
One might also ask generally in how far metric spaces are a adequate model for representing the relations between concepts. 
# 
dimensions expensive 

textcite:Steyvers_2005_structure argue that natural association patterns of words exhibit small world structure and power-law degree distributions, which are better represented by a network than by inheritance hierarchies parencite:Quillian_1988_memory or high dimensional vector spaces such as those produced by LSA. 
# 
textcite:Martin_2010_ant ~argues that the mental capacities of the human brain are severly limited.~
# 
It also seems to me that high dimensional vector spaces are cognitively expensive. 
# 
Working with high dimensional data for this project, which encompasses millions of songs, hundreds of thousands of users, and tens of thousands of genres, I also realized that the traditional ~cultural matrix~ (poetics source) model is very expensive and highly ineffective in storing such high-dimensional data. 
# 
While of course no single human brain has to deal with such a number of particular objects, the complexity arises by having to deal with a much higher amount of domains. 

~source that only relevant information is given when asked~

A network model of semantic space thus seems to provide a much more
- realistic (need quote for that)
model as it also provides a model of cognitive economy. 

~OVERLAP~
~triangle inequality~


**** metric spaces reifications
It as such seems to me that conceptual metric spaces are an analytic abstraction. 
# 
Often these are constructed from similarity jugements (~fruits example~)
# 
While they can provide insights, it seems that to varying degrees these notions have become reified. 
# 
It follows that if concepts are considered probability distribution in these feature spaces, similarity between these cannot be assessed based on metric distances, but on *feature overlap*. 
# 
Feature overlap does not treat the metric space as a space itself, but as defined by underlying attributes, the overlap of which is what leads to similarity judgements. 
# 
It is thus able to asymmetric similarity judgments (~figure Y~) as well as variation in similarity judgements given same metric distance (~figure X~). 
# 
Intransitivity can be accounted for by the fuzziness inherent in probability distributions. 



**** focus on one unit's environment: asymmetry is fine: 
mpcpherson: competition is asymmetric






* questions (list of ideas with more or less relevance)

can i test prototype vs exemplar model? 
do they imply different mechanisms for survival? 

Piazzai doesn't even use word
- exemplar/prototype view
- *atypicality*
  Hannan: equivalent to informativeness
  is about relation between object and concept
  idc much about that relation

  
reflection (Lizardo)
if i make for each genre a "spread-score": how big the tension between genres is (low for heavy metal and black metal, higher for black metal and opera) -> can average that to the genre level


~is quillian part of the classical model?~ -> look up the source that Murphy cites, duh

*inclusion fallacy* against explicitly stored hierarchies (used by Murphy):  
- robin have X -> all birds have X more believable than robins have X -> ostriches have X
- can't see how that supports computed
- is typicality effect, but typicality can be considered in hierarchy
- maybe murphy didn't consider Hannan's view yet (argues against "logical reasoning")

Murphy sees typicality effects (birds have X -> robins/ostriches have X stronger for robins) as "greatest problem for pre-stored view"

"Findings of typicality effects, intransitivity of class inclusion, and evidence from RT experiments all pose problems for the stored hierarchy view" (p.209)

*** Hannan not that bad

concepts as probability distributions instead of points

concepts have spatial extent

*** Hannan bad
cite:Smith_1981_categories metric spaces bad (p.116)

Hannan: are subconcepts transitive? yup: p.55

c.) transitive: sub(c, c') \wedge sub(c' , c'') -> sub(c, c'').
that doesn't hold with furniture, chairs, car seats

fruit counterexample: 
fruit closest to item for 17/20 -> at least 8 dimensions needed (was actually re-analysis by Tversky)


*** is classical view case of Habermaasian colonization? 


*** omnivores are genre police
cite:Goldberg_2016_span: genres less likely to die when they are liked by people who like many works, but don't like genre spanning? 


*** cite:Steyvers_2005_structure

# The Large‐Scale Structure of Semantic Networks: Statistical Analyses and a Model of Semantic Growth

models of semantic organization (hierarchies, high-dimensional vector spaces, arbitrarily structured networks) incompatible with small world structure (log distribution, scale-free) of semantic networks

omfg i'm so out of my area omfg
would have to read so much more cognitive science stuff fuuuuu

good insofar: against Quillian
also LSA bad

-> do i really want reduction to metric space?

~search for asymmetry~ -> Tversky 1977

Smith, E. E. and Medin, D. L. (1981). Categories and Concepts. Cambridge, MA: Harvard
University Press.

also already have the chicken -> bird -> animal 


*** luke smith
https://youtu.be/PnCXJn2cRf4?t=2120

* refs :ignhead:
#+Latex: \begin{sloppypar}
#+Latex: \printbibliography
#+Latex: \end{sloppypar}

** export :noexport:
#+BEGIN_SRC emacs-lisp
  (org-babel-tangle)
  (defun delete-org-comments (backend)
    (loop for comment in (reverse (org-element-map (org-element-parse-buffer)
                      'comment 'identity))
      do
      (setf (buffer-substring (org-element-property :begin comment)
                  (org-element-property :end comment))
            "")))

  (let ((org-export-before-processing-hook '(delete-org-comments)))
    (switch-to-buffer (org-latex-export-to-pdf)))
#+END_SRC

#+RESULTS:
: #<buffer /home/johannes/Dropbox/gsss/thesis/text/theory.pdf>



